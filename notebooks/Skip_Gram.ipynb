{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "GOpkG7_9HwdT"
   },
   "outputs": [],
   "source": [
    "# Define Constants and imports\n",
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import src\n",
    "from importlib import reload\n",
    "import traintracker\n",
    "\n",
    "import random\n",
    "raw_dataset_dir = \"../dataset/raw\"\n",
    "prep_dataset_dir = \"../dataset/preprocessed\"\n",
    "\n",
    "weights_dir=\"../model_weights/skipgram\"\n",
    "train_tracker_path=\"../train_tracker\"\n",
    "\n",
    "train_data_dir=\"../train_tracker\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "oDXvixkEHwdF"
   },
   "source": [
    "# Introduction\n",
    "In this notebook, I'll  implement the [Word2Vec algorithm](https://en.wikipedia.org/wiki/Word2vec) using the skip-gram architecture as a starter step for sentiment analysis creating first the words embeddings then use these embeddings for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "UX8M3nOtHwdY"
   },
   "source": [
    "# Data Loading\n",
    "- load and view the reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PT955Gt7Hwda",
    "outputId": "5f937195-6d07-44b5-ca22-2c01a26e3eba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['labels.txt', 'movie.csv', 'reviews.txt', 'text8']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(raw_dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zM_ZEv0IHwde"
   },
   "source": [
    "- reviews are line separated and the reviews also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# saving labels to txt file\n",
    "with open(os.path.join(raw_dataset_dir, \"labels.txt\"), 'r',encoding=\"utf-8\") as leables_file:\n",
    "    raw_labels_txt=leables_file.read()\n",
    "with open(os.path.join(raw_dataset_dir, \"reviews.txt\"), 'r',encoding=\"utf-8\") as reviews_file:\n",
    "    raw_reviews_txt=reviews_file.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeT-UIjbStcH"
   },
   "source": [
    "## view words freqencies\n",
    "- show word freqencies before any preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OEm_T9IjHwdk"
   },
   "outputs": [],
   "source": [
    "words_counter = Counter(raw_reviews_txt.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aI-O-mdwHwdl",
    "outputId": "4282e300-97d0-4634-bffc-d55e6a09d6e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of unique words=381542\n",
      "no of words =9253570\n"
     ]
    }
   ],
   "source": [
    "words_df = pd.DataFrame(words_counter.items(), columns=['word', 'count'])\n",
    "words_df.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "print(f\"no of unique words={len(words_df)}\\nno of words ={words_df['count'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "451azq85Hwdm",
    "outputId": "f61a202a-dd57-437b-af6d-59491dffef88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  word   count\n8                  the  455003\n51                   a  246023\n6                  and  242041\n53                  of  226965\n27                  to  209659\n...                ...     ...\n198462  policewoman's,       1\n198464         Palusky       1\n198465           Kayle       1\n198466         Timler,       1\n381541         \"Chains       1\n\n[150 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>the</td>\n      <td>455003</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>a</td>\n      <td>246023</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>and</td>\n      <td>242041</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>of</td>\n      <td>226965</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>to</td>\n      <td>209659</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>198462</th>\n      <td>policewoman's,</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>198464</th>\n      <td>Palusky</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>198465</th>\n      <td>Kayle</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>198466</th>\n      <td>Timler,</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>381541</th>\n      <td>\"Chains</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 50 from top , mid and end\n",
    "src.data_preprocessing.get_head_mid_tail(words_df,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "abLEbYtmHwdn",
    "outputId": "b2d7ad28-cd65-4f6c-ea17-0f2d4d3914c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               count\ncount  381542.000000\nmean       24.253084\nstd      1240.617558\nmin         1.000000\n25%         1.000000\n50%         1.000000\n75%         3.000000\nmax    455003.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>381542.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>24.253084</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1240.617558</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>455003.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Ye9Q6QlJHwdo"
   },
   "source": [
    "- high standard deviation due the usage of words like 'the' and using punctuations in words make them counted as diffrent words\n",
    "- 75% of the words count smaller than 3\n",
    "- about 50% words appear once , may be misspelled words\n",
    "- some html tags appear in the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJBWe3Cp8JzO",
    "outputId": "1ec1e618-41d5-444a-b51d-a4e9a0fce5c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed badr\\AppData\\Local\\Temp\\ipykernel_16096\\2388422026.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  words_df.median()\n"
     ]
    },
    {
     "data": {
      "text/plain": "count    1.0\ndtype: float64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCo20nRl7Lg3"
   },
   "source": [
    "- a words with count =1 appear from the midean to the end due to existance of punctations and the txt exist in thier lower and upper case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "K4QKDwLNHwdr"
   },
   "source": [
    "# Data preprocessing\n",
    "- remove punctuations , set all chars to lowercase\n",
    "- remove noise words that appear once , that might be misspelling\n",
    "- remove high frequent words that doesn't add thing to the neighboring word\n",
    "- save the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up2n-uUA4g3D"
   },
   "source": [
    "**probability of word removal**\n",
    "- Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. \n",
    "- This process is called subsampling by Mikolov.For each word wi) in the training set, \n",
    "- we'll discard it with probability given by \n",
    "- ![p_remove1.png](./assets/p_remove1.png)![p_remove2.png](./assets/p_remove2.png)\n",
    "- p--> probability of discarding word\n",
    "- t--> threshold of word count that the probability of discarding will begin to increase from `T/t`\n",
    "- f--> frequency of a word\n",
    "- T--> total number of words\n",
    "- `set t=100 and T=500000=1e5 ` probability of removal will start to be grater than zero when the f=5000\n",
    "- ![p_remove_curve.png](./assets/p_remove_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G7feH16Tne3"
   },
   "source": [
    "## remove invalid chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rtzwLsW2Hwds"
   },
   "outputs": [],
   "source": [
    "prep_txt = src.remove_punctuations(raw_reviews_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KcxTWbM8W_0",
    "outputId": "58739785-6a19-41fa-9ce1-4b06185c89bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of unique words=160310\n",
      "no of words =9217166\n"
     ]
    }
   ],
   "source": [
    "words_counter = Counter(prep_txt.split())\n",
    "\n",
    "words_df = pd.DataFrame(words_counter.items(), columns=['word', 'count'])\n",
    "words_df.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "print(f\"no of unique words={len(words_df)}\\nno of words ={words_df['count'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9KfsAmQ8g_Y"
   },
   "source": [
    "no of unique words decreased from `381542`  to `160310` because we removed punctuation characters so word like `The. ` and `the?` will be `the` and will be counted as one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "E97v3YnWCTQT"
   },
   "outputs": [],
   "source": [
    "def cnt_ranges(df,step,col_name):\n",
    "  size=len(df)\n",
    "  df=df.sort_values(by=col_name)\n",
    "  col_idx=df.columns.get_loc(col_name)\n",
    "\n",
    "  ranges_list=[]\n",
    "  smaller_value=0\n",
    "  bigger_value=step\n",
    "\n",
    "  df_idx=0\n",
    "  while(df_idx<size):\n",
    "    cnt=0\n",
    "    while(df_idx<size and df.iloc[df_idx,col_idx]<=bigger_value):\n",
    "      cnt+=1\n",
    "      df_idx+=1\n",
    "    if cnt>0:\n",
    "      ranges_list.append([f\"{smaller_value} to {bigger_value}\",cnt])\n",
    "    smaller_value+=step\n",
    "    bigger_value+=step\n",
    "  return ranges_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "B5NE4FydFMFk"
   },
   "outputs": [],
   "source": [
    "r=cnt_ranges(words_df,20,'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "uMYOXxvhHH9T",
    "outputId": "c8b53eb6-3ea8-4eb1-ee9d-f96419858800"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                range     cnt\n0             0 to 20  142824\n1            20 to 40    6260\n2            40 to 60    2771\n3            60 to 80    1526\n4           80 to 100    1059\n..                ...     ...\n397  213640 to 213660       1\n398  230740 to 230760       1\n399  256820 to 256840       1\n400  256900 to 256920       1\n401  531180 to 531200       1\n\n[402 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>range</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0 to 20</td>\n      <td>142824</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20 to 40</td>\n      <td>6260</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>40 to 60</td>\n      <td>2771</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>60 to 80</td>\n      <td>1526</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>80 to 100</td>\n      <td>1059</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>213640 to 213660</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>230740 to 230760</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>256820 to 256840</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>400</th>\n      <td>256900 to 256920</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>401</th>\n      <td>531180 to 531200</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>402 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(r,columns=[\"range\",'cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "cqQJak7h9M0V",
    "outputId": "de12b437-d589-4a30-8084-60fdbe0509c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                        word   count\n8                        the  531191\n6                        and  256916\n47                         a  256838\n49                        of  230759\n23                        to  213654\n...                      ...     ...\n90777           pointyfinger       1\n90776      handymancarpenter       1\n90775              paragonbr       1\n90774   signorelliscreenplay       1\n160309           fishermenbr       1\n\n[300 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>the</td>\n      <td>531191</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>and</td>\n      <td>256916</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>a</td>\n      <td>256838</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>of</td>\n      <td>230759</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>to</td>\n      <td>213654</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>90777</th>\n      <td>pointyfinger</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>90776</th>\n      <td>handymancarpenter</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>90775</th>\n      <td>paragonbr</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>90774</th>\n      <td>signorelliscreenplay</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>160309</th>\n      <td>fishermenbr</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.get_head_mid_tail(words_df,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhcCJIHsRBth"
   },
   "source": [
    "##  word importance by pos/neg apperance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FdKrTzlM37q",
    "outputId": "25c5bb28-ce55-4ad0-9675-252cd9f87cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_val -4.127134385045092\n",
      "max_val 4.727387818712341\n"
     ]
    }
   ],
   "source": [
    "words_pos_neg_ratio = src.get_word_importance(prep_txt, labels=raw_labels_txt)\n",
    "                                                                           \n",
    "min_val,max_val=0,-1\n",
    "\n",
    "for word,ratio in words_pos_neg_ratio.items():\n",
    "    min_val=min(ratio,min_val)\n",
    "    max_val=max(ratio,max_val)\n",
    "print(f\"min_val {min_val}\")\n",
    "print(f\"max_val {max_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "XPsoBpUwsryI",
    "outputId": "d73f3101-5787-4692-8ce2-de4561db0cd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'I grew up (b. 1965) watching and loving the Thunderbirds. All my mates at school watched. We played \"Thunderbirds\" before school, during lunch and after school. We all wanted to be Virgil or Scott. No one wanted to be Alan. Counting down from 5 becam'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_reviews_txt[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "KQOgjGlUshSP",
    "outputId": "e1948ebf-b6c8-4580-a926-b4a58019d8e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'i grew up b 1965 watching and loving the thunderbirds all my mates at school watched we played thunderbirds before school during lunch and after school we all wanted to be virgil or scott no one wanted to be alan counting down from 5 became an art fo'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_txt[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YBQV8D4rrJSV"
   },
   "outputs": [],
   "source": [
    "words_p_n_ratio_df=pd.DataFrame(words_pos_neg_ratio.items(),columns=['word','ratio']).sort_values(by=['ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "mT4klynyuBsi",
    "outputId": "fcc6b8ef-e4c0-4a1e-9822-e24a94506b20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               ratio\ncount  161193.000000\nmean        0.048999\nstd         0.818501\nmin        -4.127134\n25%        -0.693147\n50%         0.000000\n75%         0.693147\nmax         4.727388",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>161193.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.048999</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.818501</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-4.127134</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.693147</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.727388</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_p_n_ratio_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByY-YMrYuXJG"
   },
   "source": [
    "**Deciding the Ratio for noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0214xpXM37q",
    "outputId": "daa5b995-64d2-41a4-c617-c3f58c9a5aed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common words\n",
      "pos and negative ratio for 'the' 0.15040138861478275\n",
      "pos and negative ratio for 'him' 0.23241749635693773\n",
      "pos and negative ratio for 'her' 0.24583923387574738\n",
      "pos and negative ratio for 'is' 0.12302609115914889\n",
      "pos and negative ratio for 'they' -0.2687558382621351\n",
      "pos and negative ratio for 'did' -0.2160042141353108\n",
      "pos and negative ratio for 'as' 0.23724039177675732\n",
      "pos and negative ratio for 'a' 0.037063342877560294\n",
      "\n",
      "Postive words\n",
      "pos and negative ratio for 'wonderful' 1.5571694838883434\n",
      "pos and negative ratio for 'amazing' 1.343095156017315\n",
      "pos and negative ratio for 'good' 0.005656862045491339\n",
      "pos and negative ratio for 'excellent' 1.4842294169777621\n",
      "\n",
      "negative words\n",
      "pos and negative ratio for 'bad' -1.362353199221966\n",
      "pos and negative ratio for 'worse' -1.6799782168127269\n",
      "pos and negative ratio for 'boring' -1.5131861129745143\n"
     ]
    }
   ],
   "source": [
    "print(\"common words\")\n",
    "print(f\"pos and negative ratio for 'the' {words_pos_neg_ratio['those']}\")\n",
    "print(f\"pos and negative ratio for 'him' {words_pos_neg_ratio['him']}\")\n",
    "print(f\"pos and negative ratio for 'her' {words_pos_neg_ratio['her']}\")\n",
    "print(f\"pos and negative ratio for 'is' {words_pos_neg_ratio['is']}\")\n",
    "print(f\"pos and negative ratio for 'they' {words_pos_neg_ratio['they']}\")\n",
    "print(f\"pos and negative ratio for 'did' {words_pos_neg_ratio['did']}\")\n",
    "print(f\"pos and negative ratio for 'as' {words_pos_neg_ratio['as']}\")\n",
    "print(f\"pos and negative ratio for 'a' {words_pos_neg_ratio['a']}\")\n",
    "print(\"\\nPostive words\")\n",
    "\n",
    "print(f\"pos and negative ratio for 'wonderful' {words_pos_neg_ratio['wonderful']}\")\n",
    "print(f\"pos and negative ratio for 'amazing' {words_pos_neg_ratio['amazing']}\")\n",
    "print(f\"pos and negative ratio for 'good' {words_pos_neg_ratio['good']}\")\n",
    "print(f\"pos and negative ratio for 'excellent' {words_pos_neg_ratio['excellent']}\")\n",
    "\n",
    "print(\"\\nnegative words\")\n",
    "print(f\"pos and negative ratio for 'bad' {words_pos_neg_ratio['bad']}\")\n",
    "print(f\"pos and negative ratio for 'worse' {words_pos_neg_ratio['worse']}\")\n",
    "print(f\"pos and negative ratio for 'boring' {words_pos_neg_ratio['boring']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Tbm2QKtote0v"
   },
   "outputs": [],
   "source": [
    "ratio_threshold=0.27\n",
    "is_common_word=(words_p_n_ratio_df['ratio']>-ratio_threshold)&(words_p_n_ratio_df['ratio']<ratio_threshold)\n",
    "is_positive_word=(words_p_n_ratio_df['ratio']>ratio_threshold)\n",
    "is_negative_word=(words_p_n_ratio_df['ratio']<-ratio_threshold)\n",
    "\n",
    "\n",
    "common_words=words_p_n_ratio_df[is_common_word].sort_values(by='ratio')\n",
    "\n",
    "positive_words=words_p_n_ratio_df[is_positive_word].sort_values(by='ratio')\n",
    "\n",
    "negative_words=words_p_n_ratio_df[is_negative_word].sort_values(by='ratio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNfxFa31t42R",
    "outputId": "be155deb-8f4e-475c-c991-c61ffadb2c10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common words  18124\n",
      "positive words  75654\n",
      "negative words  67415\n"
     ]
    }
   ],
   "source": [
    "print(\"common words \",common_words['word'].count())\n",
    "print(\"positive words \",positive_words['word'].count())\n",
    "print(\"negative words \",negative_words['word'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "hLc62Fb6yU7A",
    "outputId": "2e02592b-b1f2-400d-a174-c68d9497922a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          word     ratio\n3101      foot -0.269664\n813     filthy -0.269664\n45       would -0.269380\n5529    bright -0.269255\n2076      hide -0.269129\n...        ...       ...\n18671   downey  0.269664\n19720     1986  0.269664\n1528      told  0.269740\n369    certain  0.269763\n2939     entry  0.269824\n\n[900 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3101</th>\n      <td>foot</td>\n      <td>-0.269664</td>\n    </tr>\n    <tr>\n      <th>813</th>\n      <td>filthy</td>\n      <td>-0.269664</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>would</td>\n      <td>-0.269380</td>\n    </tr>\n    <tr>\n      <th>5529</th>\n      <td>bright</td>\n      <td>-0.269255</td>\n    </tr>\n    <tr>\n      <th>2076</th>\n      <td>hide</td>\n      <td>-0.269129</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18671</th>\n      <td>downey</td>\n      <td>0.269664</td>\n    </tr>\n    <tr>\n      <th>19720</th>\n      <td>1986</td>\n      <td>0.269664</td>\n    </tr>\n    <tr>\n      <th>1528</th>\n      <td>told</td>\n      <td>0.269740</td>\n    </tr>\n    <tr>\n      <th>369</th>\n      <td>certain</td>\n      <td>0.269763</td>\n    </tr>\n    <tr>\n      <th>2939</th>\n      <td>entry</td>\n      <td>0.269824</td>\n    </tr>\n  </tbody>\n</table>\n<p>900 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words=300\n",
    "common_words_range=src.get_head_mid_tail(common_words,n_words)\n",
    "positive_words_range=src.get_head_mid_tail(positive_words,n_words)\n",
    "negative_words_range=src.get_head_mid_tail(negative_words,n_words)\n",
    "\n",
    "common_words_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Acgs1hVmRRHK"
   },
   "outputs": [],
   "source": [
    "# add exceptional words\n",
    "excep=['story','movie','good','film','movies']\n",
    "for word in excep:\n",
    "  words_pos_neg_ratio[word]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6BT_ySzRRmQ"
   },
   "source": [
    "## remove words with high and low freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iLO-OEjyInFq",
    "outputId": "6a44c5b3-d8c2-496c-8989-4b855b90d5a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'src.data_preprocessing' from 'D:\\\\2022 acheivments\\\\Projects\\\\DeepLearning NanoDegree\\\\sentiment-analysis-RNN\\\\src\\\\data_preprocessing.py'>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<module 'src.data_preprocessing' from 'D:\\\\2022 acheivments\\\\Projects\\\\DeepLearning NanoDegree\\\\sentiment-analysis-RNN\\\\src\\\\data_preprocessing.py'>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.data_preprocessing as prep\n",
    "reload(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "v3_VON9zHwdt"
   },
   "outputs": [],
   "source": [
    "filtered_prep_txt, noise_words,prob_drop_dist = prep.remove_noise(prep_txt, p_drop_dist_threshold=1e4, min_prob_drop=0.8, min_freq=20,min_rev_freq=5,common_words_threshold=0.27,word_importance_dict=words_pos_neg_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "BZEfeT-0JMQo"
   },
   "outputs": [],
   "source": [
    "prob_drop_word_df=pd.DataFrame(prob_drop_dist.items(),columns=[\"word\",'probabilty of drop']).sort_values(by=['probabilty of drop'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "fdRJR7bDJkKo",
    "outputId": "0d93b4fd-c779-4dab-a32e-5071a3bf67ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        word  probabilty of drop\n8        the            0.958344\n6        and            0.940103\n47         a            0.940094\n49        of            0.936800\n23        to            0.934318\n...      ...                 ...\n1447     saw            0.570905\n502    right            0.570648\n1006  almost            0.568963\n410     come            0.566377\n600     must            0.565623\n\n[150 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>probabilty of drop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>the</td>\n      <td>0.958344</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>and</td>\n      <td>0.940103</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>a</td>\n      <td>0.940094</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>of</td>\n      <td>0.936800</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>to</td>\n      <td>0.934318</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1447</th>\n      <td>saw</td>\n      <td>0.570905</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>right</td>\n      <td>0.570648</td>\n    </tr>\n    <tr>\n      <th>1006</th>\n      <td>almost</td>\n      <td>0.568963</td>\n    </tr>\n    <tr>\n      <th>410</th>\n      <td>come</td>\n      <td>0.566377</td>\n    </tr>\n    <tr>\n      <th>600</th>\n      <td>must</td>\n      <td>0.565623</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_drop_word_df.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FrFSfSLHwdt",
    "outputId": "cd4615e8-996c-4083-cb40-36a617aa5c09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['reticent',\n 'hilaraious',\n 'trashcult',\n 'tovah',\n 'radius',\n 'frankenstien',\n 'moviestory',\n 'childrensfamily',\n 'realizeshe',\n 'lovejust',\n 'youwatch',\n 'aardvark',\n 'languagebrief',\n 'crapand',\n 'eversocareful',\n 'feebleminded',\n 'apoplectic',\n 'speedsbr',\n 'reinforcements',\n 'wasabi',\n 'annick',\n 'psychoteacher',\n 'walshs',\n 'itbuy',\n 'goddaughter',\n 'famerodgers',\n 'failded',\n 'doxy',\n 'japaneseamerican',\n '8090',\n 'demonization',\n 'greying',\n 'tacking',\n 'abercrombie',\n 'combed',\n 'bleh',\n 'heartfc',\n 'timberlakes',\n 'stupidy',\n 'hesitating',\n 'moviehated',\n 'beautybr',\n 'gawky',\n 'dutrone',\n 'indolently',\n 'nordby',\n 'logicbut',\n 'lymi',\n 'stomachssickening',\n 'completionist',\n 'enzyme',\n 'costumingbr',\n 'englishlinear',\n 'unhip',\n 'openair',\n 'vendettabr',\n 'shouldthats',\n 'prefeature',\n 'pouts',\n 'daylightsounds',\n 'obout',\n 'it',\n 'whiteout',\n 'baggy',\n 'snowmans',\n 'schade',\n 'slits',\n 'megadose',\n 'fritzs',\n 'bigcast',\n 'mindframe',\n 'zaitochi',\n 'verhoevenbr',\n 'meantimebr',\n 'ongbak',\n 'pacingwhich',\n 'tex',\n 'fatima',\n 'ultramodern',\n 'caplan',\n 'stauntonbr',\n 'poststar',\n 'panicking',\n 'swasa',\n 'bellemaman',\n 'powersthe',\n 'coombes',\n 'unconscionable',\n 'dantzig',\n 'whitehallbr',\n 'callsmeanwhile',\n 'repoters',\n 'quesy',\n '4550',\n 'caseour',\n 'akins',\n 'judds',\n 'cheddarfest',\n 'crownedbr',\n 'nuculer',\n 'apparrently',\n 'dawn7',\n 'womenunfriendly',\n 'township',\n 'brenners',\n 'fopbr',\n 'krajobraz',\n 'physiology',\n 'argentoplenty',\n 'stales',\n 'exsergeantmajor',\n 'briefcasebr',\n 'coorainetc',\n 'trafficked',\n 'filmstiftung',\n 'condescendant',\n 'abhorrent',\n 'bopping',\n 'inexorable',\n 'brunch',\n 'sidestories',\n 'boysexaggerating',\n 'moives',\n 'bankotsu',\n 'superdaring',\n 'yeaworth',\n 'marnac',\n 'whih',\n 'soupÃ§on',\n 'amenenough',\n 'anythingthe',\n 'apollos',\n 'alterio',\n 'lariviere',\n 'koestlers',\n 'bitespecially',\n 'commonbr',\n 'oncelofty',\n 'twistedly',\n 'quantitative',\n 'dencik',\n 'shirelles',\n 'nuremburg',\n 'httprogerebertsuntimescomappspbcsdllsectioncategoryanswerman',\n 'antidepression',\n 'rapee',\n 'inenjoying',\n 'renderings',\n 'interlenghi',\n 'reaccounting',\n 'completelybut',\n 'almedia',\n 'meritsthe',\n 'femmefatales',\n '2425',\n 'reginal',\n 'trending',\n 'japon',\n 'daff',\n 'missfires',\n 'proscribed',\n 'privately',\n 'tolan',\n 'snoorefest',\n 'betoday',\n 'annihilationarmageddon',\n 'strawberry',\n 'foregrounds',\n 'alibibr',\n 'etchedthe',\n 'phillipglasslike',\n 'jarrett',\n 'egging',\n 'typea',\n 'analyst',\n 'telescopes',\n 'shudnt',\n 'exaggerationbr',\n 'imagesincluding',\n 'bintang',\n 'snipesnot',\n 'sierrabr',\n 'spt11',\n 'awwwwbr',\n 'zaniness',\n 'speaksnot',\n 'vampyres',\n 'technocracy',\n 'coronary',\n 'yb',\n 'faraj',\n 'cranston',\n 'bgore',\n 'inneremotions',\n 'asthma',\n 'toddlers',\n 'forgiveable',\n 'peipus',\n 'manonly',\n '7eventy',\n 'testi',\n 'bassiano',\n 'jarchovskÃ½',\n 'myselfwow',\n 'beits',\n 'comesup',\n 'miroku',\n 'lithely',\n 'incidentals',\n 'sixes',\n 'bradys',\n 'disengages',\n 'costeffective',\n 'askedbr',\n 'guidebut',\n 'hackwork',\n 'enclosure',\n 'crushes',\n 'chimp\\x97with',\n 'puncturing',\n 'upeventually',\n 'edera',\n 'telkovsky',\n 'clamshells',\n 'timeetc',\n 'marmont',\n 'deadites',\n 'filmpornbr',\n 'dulling',\n 'bullwhipbr',\n 'ellisons',\n 'saltimbanco',\n 'fastly',\n 'capitalthe',\n 'flinty',\n 'sacrilegiously',\n '1and',\n 'yummo',\n 'interruptingbr',\n 'keeleybr',\n 'halebr',\n 'eurodisco',\n 'selfrepetitive',\n 'usualthe',\n 'larroquette',\n 'budgetbut',\n 'washoe',\n 'lazerus',\n 'muchmuch',\n 'yadavs',\n 'priveleged',\n 'vcddvd',\n 'mangafighterstyle',\n 'suchandsuch',\n 'reemergence',\n 'kÃ´hÃ®',\n 'turkeyzero',\n 'mata',\n 'electoral',\n 'quickfire',\n 'onelinear',\n 'markwhere',\n '505',\n 'dehumanizing',\n 'formulaicboring',\n 'chancer',\n 'easybut',\n 'cinemasbr',\n '640',\n 'mconaughey',\n 'nagamatsu',\n 'jordache',\n 'suitplayed',\n 'glazing',\n 'panto',\n 'californiaeven',\n 'zonebr',\n 'carrollbr',\n 'edmednson',\n 'manycolored',\n 'loanouts',\n 'danceorgy',\n 'lazslo',\n 'learytolan',\n 'splicedin',\n 'wittenborn',\n '\\x91true',\n 'manhattanites',\n 'spellings',\n 'decieve',\n 'thenthey',\n 'smillas',\n 'nortons',\n 'fightsceneending',\n 'lynchstyle',\n 'perversity',\n 'grandmotherbr',\n 'honkers',\n '6070',\n 'movieseen',\n '8bit',\n 'joycey',\n 'sandys',\n 'denethor',\n 'audiencedone',\n 'multireligion',\n 'huntingkilling',\n 'petaluma',\n 'bowlo',\n 'prerunner',\n 'sujeong',\n 'swordknife',\n 'sayprobably',\n 'hipotetic',\n 'sorrysouth',\n 'unrepentant',\n 'worsti',\n 'egreens',\n 'freshmen',\n 'technocompany',\n 'jlo',\n 'hologram',\n 'mcarthur',\n 'newsprint',\n 'pseudodetective',\n 'kingangle',\n 'mephisto',\n '730pm',\n 'alltooconveniently',\n 'nutsand',\n 'dresdenas',\n 'hawaiialthough',\n 'asleepit',\n 'slowfingers',\n 'bayhusband',\n 'immigrantsbr',\n 'revved',\n 'aheadthis',\n 'cableborn',\n 'ramboesque',\n 'bortz',\n 'motownespecially',\n 'lookthe',\n 'frollo',\n 'americanas',\n 'considerationsbr',\n 'yoitll',\n 'illbehaved',\n 'selick',\n 'nemisis',\n 'scarethis',\n 'freshener',\n 'afgan',\n 'blackblacks',\n 'akemi',\n 'terriblethis',\n 'finalizing',\n 'blurting',\n 'aasize',\n 'advisor',\n 'bloomberg',\n 'creme',\n 'blondieromantic',\n 'doktor',\n 'feeling\\x97it',\n 'scenerychewers',\n 'farmbr',\n 'robsons',\n 'nonsuperhero',\n 'mdt',\n 'flourescent',\n 'heartbeatbr',\n 'samoan',\n 'rh',\n 'woddy',\n 'singlecamera',\n 'demonized',\n 'longpausesbetweenlines',\n 'sciencefictionscriptwriter',\n 'wendsday',\n 'chiahwesons',\n 'harshest',\n 'evenly',\n 'wellhandled',\n 'restraints',\n 'dated4',\n 'rohmerbr',\n 'survivial',\n 'peabrained',\n 'miguire',\n 'suppressive',\n 'diculous',\n 'filmbynumbers',\n 'debunk',\n 'xxth',\n 'sebastians',\n 'romanianmade',\n 'gomi',\n 'albas',\n 'inuyashabr',\n 'torturer',\n 'moppets',\n 'sluzbenom',\n 'jenkinsbirthday',\n 'psychicoccult',\n 'stepfamily',\n 'southerner',\n 'overproduced',\n 'kazzam',\n 'genrecult',\n 'colourised',\n 'truelife',\n 'warmfuzzy',\n 'sioux',\n 'herinteractive',\n 'createdthats',\n 'whook',\n 'obliviona',\n 'lucastaanother',\n '1963the',\n 'drano',\n 'ernests',\n 'linbaker',\n 'halloweennight',\n 'industrialization',\n 'mortuary',\n 'bigon',\n 'undermanned',\n '\\x91in',\n 'kapows',\n 'dogeatdog',\n 'bimbette',\n 'thurig',\n 'tomeibr',\n 'sking',\n 'targetthey',\n 'editingmusicby',\n 'libyan',\n 'loachs',\n 'falkon',\n 'karls',\n 'sleazyloser',\n 'luckyly',\n 'octane',\n 'notsoperfecthusband',\n '45s',\n 'andwellhilarity',\n 'traitors',\n 'misshapenlyfeatured',\n 'sentinels',\n 'effortstephen',\n 'shawnhams',\n 'hotz',\n 'wanderings',\n 'poorlybr',\n 'singhs',\n 'obstructs',\n 'vineswinging',\n 'roden',\n 'xiong',\n 'weasel100',\n 'bertie',\n 'jerkbr',\n 'friÃ°rik',\n 'gummi',\n 'faithstrenghtand',\n 'celluloidthe',\n 'minilevels',\n 'multiseason',\n 'beyondand',\n 'rolevery',\n 'writerdirectorproducer',\n 'cartoonishcgi',\n 'ockerishness',\n 'kismet',\n 'borrowings',\n 'brands',\n 'excercise',\n 'coercedbr',\n 'budged',\n 'directedbr',\n 'daness',\n 'beakers',\n 'coronerone',\n 'shigematsu',\n 'tecla',\n 'gangsterall',\n 'lottt',\n 'shmaltz',\n 'grumbled',\n 'nowobvious',\n 'judgebr',\n 'shamed',\n 'pushtun',\n 'heregalaxina',\n 'suttersville',\n 'halfporno',\n 'knotbr',\n 'gettinf',\n 'jeers',\n 'svensk',\n 'attainment',\n 'strathairn',\n 'greggs',\n 'carmon',\n 'muslimthis',\n 'unquiet',\n 'nature\\x97or',\n '12383499143743701',\n 'ticlaws',\n 'eversmiling',\n 'deflates',\n 'exfighter',\n 'herculis',\n 'retardedbr',\n 'selfdelusionbr',\n '1415yearold',\n 'thrillerwith',\n 'vulgarbr',\n 'fedevich',\n 'hightly',\n 'trejos',\n 'slavestates',\n 'accountsbr',\n 'clergyman',\n 'ahhhhhha',\n 'reallike',\n 'prendergast',\n 'wantedhe',\n 'belannas',\n 'rebound',\n 'compute',\n 'corcoran',\n 'dossiers',\n 'beller',\n 'inneundo',\n 'crazies',\n 'entertainmentand',\n 'tinto',\n 'wreckyou',\n 'dunnigan',\n 'chaykin',\n 'sleuths',\n 'nottherefore',\n 'kittson',\n 'hopscotches',\n 'meltzer',\n '8Â½',\n 'miramar',\n 'toyrobot',\n 'dummiesbr',\n 'betti',\n 'rebreather',\n 'alaswhat',\n 'eeewwww',\n 'shiri',\n 'saira',\n 'rullet',\n 'airphone',\n 'dre',\n 'thiswhat',\n 'sarda',\n 'kellie',\n 'centurythe',\n 'hummel',\n 'termination',\n 'astonishingbr',\n '\\x84raves',\n 'communalist',\n 'killedthe',\n 'slightlybut',\n 'dullsville',\n 'wield',\n 'tims',\n 'actedbr',\n 'neros',\n 'mounties',\n 'normals',\n 'invulnerable',\n 'sicne',\n 'fellowmens',\n 'affaire',\n 'mayble',\n 'fetishised',\n 'chacun',\n 'lefts',\n 'zipper',\n 'baracus',\n 'ministro',\n 'drsuess',\n 'accompanist',\n '9pmbr',\n 'oss117whereas',\n 'medina',\n 'jockboyfriend',\n 'tabooishbr',\n 'oboyle',\n 'lizitis',\n 'duckofdeath',\n 'lessthanordinary',\n 'woodsy',\n 'finalitywith',\n 'kuei',\n 'foreveri',\n 'youso',\n 'jÃ¤niksen',\n 'oowaah',\n 'scrutinising',\n 'fitfully',\n 'chuckieplayed',\n 'schumanns',\n 'thinkingbehaving',\n 'hollowed',\n 'onealmost',\n 'spearheads',\n 'traceable',\n 'hooter',\n 'sentimentbr',\n 'wardenbr',\n 'herthis',\n 'megyn',\n 'superficious',\n 'lashelle',\n 'becauseand',\n '4shaolin',\n 'rollercoster',\n 'prÃ¨s',\n 'sabadell',\n 'trini',\n 'roleshe',\n 'prewatergate',\n 'lifeas',\n 'undertitles',\n 'coppy',\n 'vangard',\n 'barkerbr',\n 'excomedian',\n 'horrorfilled',\n 'detonador',\n 'transformative',\n 'nigserian',\n 'gisela',\n 'careerending',\n 'davisplaying',\n 'carribien',\n 'indecision',\n 'persevered',\n 'lamy',\n 'performanceserika',\n 'monahans',\n 'othersnot',\n '2do',\n 'revivedrebroadcast',\n 'moonlighting\\x97that',\n 'slaone',\n 'manthough',\n 'gaymode',\n 'alissia',\n 'okbut',\n 'ammmmmbbbererrrrrrrrrgerrrrrrrssss',\n 'babu',\n 'actionwar',\n 'allocated',\n 'latergiving',\n 'donor',\n 'illiteracybr',\n 'johannas',\n 'dollari',\n 'enforce',\n 'imagery\\x97and',\n 'pampers',\n 'perversly',\n 'banditi',\n 'soninlaws',\n 'byebye',\n 'tatsuya',\n 'matshelge',\n 'pennypinch',\n 'treehuggers',\n 'watchread',\n 'inebriation',\n 'blackedup',\n 'chicagoans',\n 'lampidorrans',\n 'longforgotten',\n 'vivans',\n 'manquÃ©',\n 'activated',\n 'satyajit',\n 'ipolite',\n 'cubemovie',\n 'weismullers',\n 'bosspartner',\n 'cremedelacreme',\n 'doiiing',\n 'jaden',\n 'sleepi',\n 'additive',\n 'sobadthatitisgood',\n 'universethe',\n 'bewith',\n 'lamo',\n '24k',\n 'centerline',\n 'missunderstood',\n 'nonrpers',\n 'macbook',\n 'avalonannette',\n 'oftnoted',\n 'likingdeserved',\n 'lofi',\n 'oceaninfested',\n 'mclaglengypo',\n 'welloiled',\n 'beheld',\n 'equitable',\n 'fillintheblanks',\n 'interlinking',\n 'shescientist',\n '4time',\n 'dimwittedbr',\n 'caponebr',\n 'tatsuo',\n 'theism',\n 'jungwons',\n 'priggish',\n 'dillonbr',\n 'adventuredont',\n 'minha',\n 'makhmalbafs',\n 'rrw',\n 'paedogeddon',\n 'writersproducersdirector',\n 'titlesintro',\n 'humpp',\n 'whittington',\n 'wung',\n 'centerfold',\n 'dyrholm',\n 'luisi',\n 'semimajor',\n 'buÃ±uelbr',\n 'outselling',\n 'manoux',\n 'slurrings',\n 'tranquillo',\n 'frightfactor',\n 'zÃ¼bert',\n 'impertinence',\n 'hongryun',\n 'headengulfedbydemonvagina',\n 'pickbr',\n 'oddballquirky',\n 'violentlybr',\n 'grudgefilms',\n 'sollett',\n 'leastfor',\n 'admirablybr',\n 'wingsone',\n 'edmonton',\n 'drugandmoney',\n 'uncoloured',\n 'deceaseds',\n 'protean',\n 'alltoofamiliar',\n 'horseman',\n 'scaled',\n 'kya',\n 'lames',\n 'voicing',\n 'moviemixing',\n 'joner',\n 'indiantraders',\n 'scheduleservletactiondetaildetailfocusid598947',\n 'marshes',\n 'lauper',\n 'schenkel',\n 'bigbreasted',\n 'albumsbr',\n 'soukup',\n 'lumierefamily',\n 'romulus',\n '2007\\x97two',\n 'comparisonsthe',\n 'sermonbr',\n 'tonguelashing',\n 'ikearmand',\n 'shyster',\n 'beeds',\n 'spillane',\n 'lanford',\n 'kicka',\n 'awwwww',\n 'trivialityand',\n 'pittjohnny',\n 'lipstickbr',\n 'waitingi',\n 'jiah',\n 'goodlaughed',\n 'massaria',\n 'exploitationsickie',\n 'rocketsbr',\n 'fledgling',\n 'videomakers',\n 'thisfilmwas',\n 'bryce',\n 'inuiyasha',\n 'lupone',\n 'angrysadcrying',\n 'mobarakshahi',\n 'apprehensiveness',\n 'somewhatsimilar',\n 'fluctuationsbr',\n 'haveterrible',\n 'rehasha',\n 'daves',\n 'australasian',\n 'actingdirection',\n 'generationscause',\n 'cockchasing',\n 'alper',\n 'gob',\n 'cellarbr',\n 'scuffy',\n 'semiinterior',\n 'saiff',\n 'nonunion',\n 'marlows',\n 'classicwill',\n 'sabrinas',\n 'wellas',\n 'nobu',\n 'crediblebr',\n 'societyculture',\n 'washerwoman',\n 'reporteranchorjournalist',\n 'jarndyce',\n 'vampirefangs',\n 'decidedi',\n 'godiva',\n 'onereeler',\n 'postlethwaite',\n 'steeeeeeriiiiiike',\n 'umbrillo',\n 'anoes',\n 'straightfromtheshoulder',\n 'lyndsay',\n 'two2',\n 'rudeim',\n 'whereagainst',\n 'womeninprison',\n 'quasigothic',\n 'rolodex',\n 'annemoss',\n 'moorican',\n 'mangiamatti',\n 'valhalla',\n 'melo',\n 'spiritedness',\n 'popes',\n 'mvovies',\n 'antonionibr',\n 'gillespie',\n 'altaira',\n 'baylinda',\n 'reboot',\n 'tightfitting',\n 'thisever',\n 'trvor',\n 'smitheebr',\n 'wozzeck',\n 'superfighters',\n 'pupi',\n 'rÃ¥zone',\n 'laughsvery',\n 'oldmovie',\n 'spotat',\n 'undertakebr',\n 'sikkim',\n 'tinglingly',\n 'suuuuure',\n 'wizto',\n 'unintelligble',\n 'kamikaze',\n 'postvietnam',\n 'imagesthis',\n 'woodenand',\n 'bagels',\n 'rageemotional',\n 'jarvas',\n 'morethanclichÃ©',\n 'stagers',\n 'fernack',\n 'osterns',\n 'yakins',\n 'fiving',\n 'cloudscape',\n 'bigstudio',\n 'itunderstanding',\n 'wolfies',\n 'voorhees',\n 'unrivaled',\n 'predicaments',\n 'emigre',\n 'tarantulaswho',\n 'fished',\n 'amenbr',\n 'rolffes',\n 'hangerie',\n 'rysdaves',\n 'cowboysindians',\n 'fletch',\n 'backgroundsbr',\n 'smoother',\n 'purgatorybr',\n 'extremealthough',\n 'negotiable',\n 'yeahi',\n 'glassshattering',\n 'darnells',\n 'predictjust',\n 'classied',\n 'cede',\n 'rubishbr',\n 'retrofuture',\n 'mathbr',\n 'fantomas',\n 'clifftop',\n 'bodegabr',\n 'mapping',\n 'artisticly',\n 'sharkmen',\n 'jamesa',\n 'saidi',\n 'kopps',\n 'scenessequences',\n 'bavas',\n 'fieldss',\n 'perestroika',\n 'leprechaun',\n 'dvdit',\n 'latch',\n 'badboy',\n 'pedofile',\n 'spookier',\n 'interviewedesp',\n 'caledon',\n 'authorize',\n 'citiesthis',\n 'pentagrams',\n 'chagrinbr',\n 'alfalfa',\n 'coates',\n 'aaww',\n 'chambara',\n 'halfeaten',\n 'trouser',\n 'blackwhitethinking',\n 'bulldogs',\n 'friendsthey',\n 'attachmentbr',\n 'rehearsedbr',\n 'sidebyside',\n 'blessingand',\n 'kanefsky',\n 'denigration',\n 'alliesadversary',\n 'mrpoo',\n 'neutralizing',\n 'vaughanjoe',\n 'croak',\n 'mtvbr',\n 'batjac',\n 'factorsamerican',\n 'participates',\n 'montesinosfujimori',\n 'steffania',\n 'movieshes',\n 'ahet',\n 'hibernia',\n 'movieother',\n 'musici',\n 'hiker',\n 'soapieeven',\n 'rahad',\n 'performancechunky',\n 'carbash',\n 'moroccan',\n 'fillit',\n 'unsuccessfulat',\n 'jatte',\n 'novelyou',\n 'patriots',\n 'bankrupts',\n 'autoderisionbr',\n 'okumoto',\n 'letsup',\n 'jaglon',\n 'patridge',\n 'bohls',\n ...]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBL11bC6Hwdu",
    "outputId": "bb0c87cb-172c-475c-cdc4-bd66c82a8d87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142368\n"
     ]
    }
   ],
   "source": [
    "print(len(noise_words))\n",
    "prep_txt=filtered_prep_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAe-BdJCKrzW",
    "outputId": "0ebe45da-0642-48df-dc77-d77dc9a5c61b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of unique words=17942\n",
      "no of words =5130422\n"
     ]
    }
   ],
   "source": [
    "words_counter = Counter(prep_txt.split())\n",
    "\n",
    "words_df = pd.DataFrame(words_counter.items(), columns=['word', 'count'])\n",
    "words_df.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "print(f\"no of unique words={len(words_df)}\\nno of words ={words_df['count'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "NR2hi71wK40z"
   },
   "outputs": [],
   "source": [
    "r=cnt_ranges(words_df,10,'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "qqzY6A2jK400",
    "outputId": "8803cfb8-8ec8-44b8-ae73-953769a8fc23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           range   cnt\n0       10 to 20   512\n1       20 to 30  3929\n2       30 to 40  2319\n3       40 to 50  1606\n4       50 to 60  1165\n..           ...   ...\n95    960 to 970    11\n96    970 to 980     8\n97    980 to 990     4\n98   990 to 1000    17\n99  1000 to 1010    11\n\n[100 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>range</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10 to 20</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20 to 30</td>\n      <td>3929</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30 to 40</td>\n      <td>2319</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40 to 50</td>\n      <td>1606</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50 to 60</td>\n      <td>1165</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>960 to 970</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>970 to 980</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>980 to 990</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>990 to 1000</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>1000 to 1010</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(r,columns=[\"range\",'cnt']).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "yrdFbb6HHBYD",
    "outputId": "a501b2bb-e909-45a6-aaef-9c45b5123c41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             word  count\n11361       lifes     99\n6981      toronto     99\n6823     programs     99\n3690      seeming     99\n7893       akshay     99\n...           ...    ...\n17265      odious     20\n12904  excitingbr     20\n12912       sonia     20\n12929        pods     20\n7845       evilbr     20\n\n[12074 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11361</th>\n      <td>lifes</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>6981</th>\n      <td>toronto</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>6823</th>\n      <td>programs</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>3690</th>\n      <td>seeming</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>7893</th>\n      <td>akshay</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17265</th>\n      <td>odious</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>12904</th>\n      <td>excitingbr</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>12912</th>\n      <td>sonia</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>12929</th>\n      <td>pods</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>7845</th>\n      <td>evilbr</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n<p>12074 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df[words_df['count']<100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "6rgKY_aKF0MY",
    "outputId": "f61874d0-8c63-4858-bd85-3f5421fe65a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             word  count\n31          movie  66976\n62            his  45522\n129          just  28030\n128            if  26971\n95           good  22980\n...           ...    ...\n17265      odious     20\n12904  excitingbr     20\n12912       sonia     20\n12929        pods     20\n7845       evilbr     20\n\n[3000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>31</th>\n      <td>movie</td>\n      <td>66976</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>his</td>\n      <td>45522</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>just</td>\n      <td>28030</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>if</td>\n      <td>26971</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>good</td>\n      <td>22980</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17265</th>\n      <td>odious</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>12904</th>\n      <td>excitingbr</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>12912</th>\n      <td>sonia</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>12929</th>\n      <td>pods</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>7845</th>\n      <td>evilbr</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n<p>3000 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.get_head_mid_tail(words_df,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "5MQyC_AsHwdv",
    "outputId": "04e21b01-bf65-4177-f933-90fb3f63d039"
   },
   "outputs": [],
   "source": [
    "raw_reviews_list=raw_reviews_txt.split('\\n')\n",
    "prep_reviews_list=prep_txt.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "yBr3j4kYHwdx",
    "outputId": "2e8fbaac-545e-47d7-f8b9-96d719d5c3ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review before\n",
      "\n",
      "Christopher Lee is one of my favorite actors! I'm trying to view all of his work. He has been known to single-handedly save movies with his presence. Unfortunately, this is not one of them. This movie suffers from a low budget and it's production values are disturbing. Please...for the love of Christopher....avoid this film!\n",
      "\n",
      "review after\n",
      "\n",
      "christopher lee my favorite actors im trying view his work been known singlehandedly save movies his presence unfortunately them movie suffers low budget production values disturbing love \n"
     ]
    }
   ],
   "source": [
    "rand_idx=random.randint(0,len(raw_reviews_list))\n",
    "print(\"review before\\n\")\n",
    "print(raw_reviews_list[rand_idx])\n",
    "print()\n",
    "print(\"review after\\n\")\n",
    "print(prep_reviews_list[rand_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "3y7bMXXvHwdy"
   },
   "outputs": [],
   "source": [
    "# save preprocessed reviews\n",
    "with open(os.path.join(prep_dataset_dir, \"reviews.txt\"), 'w',encoding=\"utf-8\") as reviews_file:\n",
    "    reviews_file.write(prep_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "eTlNArOpHwdz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_6524n3Hwdz",
    "outputId": "188508ba-adb9-4301-becf-02e076a737fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of unique words=17943\n",
      "no of words =5189833\n"
     ]
    }
   ],
   "source": [
    "prep_words_counter = Counter(prep_txt.split())\n",
    "prep_words_df = pd.DataFrame(prep_words_counter.items(), columns=['word', 'count'])\n",
    "print(f\"no of unique words={len(prep_words_df)}\\nno of words ={prep_words_df['count'].sum()}\")\n",
    "prep_words_df.sort_values(by=['count'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8Zd43hZTHwd0",
    "outputId": "1421d115-dd4c-460d-bb4d-d496bb9f3ded"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        word  count\n31     movie  66976\n73      film  59411\n62       his  45522\n130     just  28030\n129       if  26971\n..       ...    ...\n65    should   7690\n327  through   7646\n120    those   7593\n251       im   7563\n783     back   7446\n\n[100 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>31</th>\n      <td>movie</td>\n      <td>66976</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>film</td>\n      <td>59411</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>his</td>\n      <td>45522</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>just</td>\n      <td>28030</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>if</td>\n      <td>26971</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>should</td>\n      <td>7690</td>\n    </tr>\n    <tr>\n      <th>327</th>\n      <td>through</td>\n      <td>7646</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>those</td>\n      <td>7593</td>\n    </tr>\n    <tr>\n      <th>251</th>\n      <td>im</td>\n      <td>7563</td>\n    </tr>\n    <tr>\n      <th>783</th>\n      <td>back</td>\n      <td>7446</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_words_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "REq5UxSdHwd1"
   },
   "source": [
    "- only `.` is removed with size `327192` , it was the second most frequent word , the std will decrease a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "htGeNh-XHwd1",
    "outputId": "052e8335-2284-4e0e-e032-1ce83f379b22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              count\ncount  17943.000000\nmean     289.239982\nstd     1373.175361\nmin       20.000000\n25%       31.000000\n50%       56.000000\n75%      143.000000\nmax    66976.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>17943.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>289.239982</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1373.175361</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>20.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>31.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>56.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>143.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>66976.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_words_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvzDu4GMHwd3",
    "outputId": "10980bfb-2302-49a2-db91-9df6af71ce30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "40000"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_reviews_list = prep_txt.split('\\n')\n",
    "len(prep_reviews_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NE1KBmIzHwd4"
   },
   "source": [
    "# Skip-Gram Data loader\n",
    "- Dataset Loader will load the txt data\n",
    "- count the frequency for each word and total frequency which used in selecting the noise words which doesn't appear in the word context\n",
    "- save a map for word to index and index to word , change txt to int\n",
    "- iterate over the data , return each iteration `(input,target,noise_words)` the target which is the no of words around the given word with size `w` and the input will be the word repeated `w` times\n",
    "    - ex if the txt is\n",
    "    - ` They celebrated his birthday with a big party.`\n",
    "    - and we are at the word `birthday` and the window size = 4\n",
    "    - input `[birthday,birthday,birthday,birthday]` target `[celebrated,his,with,a] `\n",
    "<br><br>\n",
    "- **selecting noise word**\n",
    "<br><br>\n",
    "- we will select from a probability distribution (probability of selecting a word as noise word) n-words\n",
    "<br><br>\n",
    "- ![noise_removal_eq1.png](./assets/noise_removal_eq1.png)\n",
    "- ![noise_removal_eq1_curve.png](./assets/noise_removal_eq1_curve.png)\n",
    "<br><br>\n",
    "- `f--> word frequency`\n",
    "- `t--> total of words frequencies`\n",
    "- with a power of `3/8` will make the probability of selecting low frequent words little higher\n",
    "<br><br>\n",
    "- ![noise_removal_eq2.png](./assets/noise_removal_eq2.png)\n",
    "- ![noise_removal_eq2_curve.png](./assets/noise_removal_eq2_curve.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1jU65tzHwd4",
    "outputId": "f2cd5360-af14-4355-c339-ed69424ba440"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "40000"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(prep_dataset_dir, 'reviews.txt'),encoding=\"utf-8\") as prep_file:\n",
    "    prep_txt = prep_file.read()\n",
    "prep_reviews_list = prep_txt.split('\\n')\n",
    "len(prep_reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "sGtYuJVLHwd5"
   },
   "outputs": [],
   "source": [
    "rev_skip_gram_data = src.Word2VecDataset(prep_txt, window_size=4, no_noise_outputs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "JqrramafP5Ee"
   },
   "outputs": [],
   "source": [
    "rev_skip_gram_data.save_word2int(prep_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "tB2RDJopPmVL"
   },
   "outputs": [],
   "source": [
    "word2intx=src.utils.load_json(os.path.join(prep_dataset_dir,\"word2int.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sc4pqohHPvC7",
    "outputId": "f037835c-738b-4a00-b977-b1758168e650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int value for amazing 444\n",
      "int value for good 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"int value for amazing {word2intx['amazing']}\")\n",
    "print(f\"int value for good {word2intx['good']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "V8CxwaDVHwd6"
   },
   "outputs": [],
   "source": [
    "itrr = iter(rev_skip_gram_data)\n",
    "word, target, noise = next(itrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5N3ZY5SHwd6",
    "outputId": "57c73a9d-dfc2-46c0-9a64-34dceef8da0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_in shape torch.Size([1366])\n",
      "target_words shape torch.Size([1366])\n",
      "noise_words shape torch.Size([1366, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"words_in shape {word.shape}\")\n",
    "print(f\"target_words shape {target.shape}\")\n",
    "print(f\"noise_words shape {noise.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHP_BakgsZiD",
    "outputId": "c4a3a12c-7ff7-43e5-eaab-30739e1bf09e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "17944"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_skip_gram_data.no_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "VAttC7_2Hwd8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# instantiating the model\n",
    "embedding_dim = 256\n",
    "model = src.SkipGram(rev_skip_gram_data.no_unique_words, embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zeQ061XuHwd8"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- feed-forward processes\n",
    "    - for window size=3 , batch_size=2\n",
    "    - txt --> `[1,2,3,4,5,6,7,8,9,10]` after being changed from txt to int\n",
    "    - take input of an index of a word repeated with window_size*2\n",
    "    - example at word 4\n",
    "    ```\n",
    "                 batch 1       batch 2\n",
    "      input  [ 4,4,4,4,4,4, 5,5,5,5,5,5 ]\n",
    "      target [ 1,2,3,5,6,7, 2,3,4,6,7,8 ]\n",
    "    ```\n",
    "- loss function\n",
    "- ![loss_function1.png](./assets/loss_function1.png)\n",
    "<br><br>\n",
    "\n",
    "- Dot product between input word and random words need to be minimized to zero\n",
    "  - ![loss_function2.png](./assets/loss_function2.png)\n",
    "<br><br>\n",
    "- selection or random words will be done using uni-gram distribution for the word\n",
    "- Dot product between input word and output word need to be maximized to be zero also\n",
    "<br><br>\n",
    "  - ![loss_function3.png](./assets/loss_function3.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show word and the closest 5 words to it before trainning\n",
    "from src import utils\n",
    "reload(utils)\n",
    "utils.closest_words(model.input_embedding,rev_skip_gram_data.int2word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "PrdtMgH7Hwd9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# using the loss that we defined\n",
    "criterion = src.NegativeSamplingLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "JGKNCZoeV6Lj"
   },
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_VpHfjKjtHl",
    "outputId": "785bad91-3196-46e0-ab84-0fa6eff5c248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model_weights/skipgram/12_07 16_45 Train_(2.84019) .pt\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_weights_path=traintracker.latest_weights_path(weights_dir)\n",
    "print(latest_weights_path)\n",
    "state_dict=torch.load(latest_weights_path,map_location='cpu')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVFiYzEoHweD",
    "outputId": "a708675e-8169-4347-c28b-c1e601dda252",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "src.skipgram_train(model=model,epochs=epochs,skip_gram_data=rev_skip_gram_data,device=device,optimizer=optimizer,criterion=criterion,train_data_dir=train_data_dir,weights_dir=weights_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- showing cosine similarity between random words after trainnig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "too | very, pretty, overly, bit, enough\n",
      "plot | storyline, story, premise, script, narrative\n",
      "how | why, really, where, mean, believe\n",
      "bad | awful, terrible, good, horrible, badbr\n",
      "two | three, four, couple, 2, 3\n",
      "such | truly, many, example, kind, great\n",
      "time | timebr, spent, week, hour, day\n",
      "love | hate, loved, friendship, loves, romantic\n",
      "convincing | believable, realistic, credible, effective, likable\n",
      "biggest | major, big, greatest, worst, huge\n",
      "intelligent | witty, smart, clever, mature, talented\n",
      "filled | full, lots, plenty, contains, loaded\n",
      "sadly | unfortunately, simply, shame, however, largely\n",
      "kills | killing, killed, kill, dies, murdering\n",
      "amount | amounts, lot, minuscule, lots, considerable\n",
      "credit | kudos, deserves, award, praise, mention\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show word and the closest 5 words to it after training\n",
    "utils.closest_words(model.input_embedding,rev_skip_gram_data.int2word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "train_intervals=pd.read_csv(train_data_dir+'/train_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "17"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_intervals)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "    epoch idx from                                 model architecture  \\\n0                1  SkipGram(\\n  (input_embedding): Embedding(4634...   \n1                1  SkipGram(\\n  (input_embedding): Embedding(4634...   \n2                1  SkipGram(\\n  (input_embedding): Embedding(1033...   \n3               17  SkipGram(\\n  (input_embedding): Embedding(2338...   \n4               27  SkipGram(\\n  (input_embedding): Embedding(2338...   \n5               30  SkipGram(\\n  (input_embedding): Embedding(2338...   \n6               40  SkipGram(\\n  (input_embedding): Embedding(3811...   \n7               51  SkipGram(\\n  (input_embedding): Embedding(3811...   \n8               52  SkipGram(\\n  (input_embedding): Embedding(3815...   \n9               52  SkipGram(\\n  (input_embedding): Embedding(3815...   \n10              77  SkipGram(\\n  (input_embedding): Embedding(2694...   \n11              88  SkipGram(\\n  (input_embedding): Embedding(1794...   \n12             107  SkipGram(\\n  (input_embedding): Embedding(1794...   \n13             112  SkipGram(\\n  (input_embedding): Embedding(1794...   \n14             114  SkipGram(\\n  (input_embedding): Embedding(1794...   \n15             114  SkipGram(\\n  (input_embedding): Embedding(1794...   \n16             128  SkipGram(\\n  (input_embedding): Embedding(1794...   \n\n    batch size                                          optimizer  \\\n0          128  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n1          512  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n2          256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n3          256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n4          512  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n5          512  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n6          256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n7          256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n8          512  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n9          512  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n10         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n11         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n12         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n13         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n14         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n15         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n16         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n\n    embedding_size  window_size  no noise samples  \\\n0              256            5                25   \n1              256            5                25   \n2              256            3                25   \n3              256            3                25   \n4              128            3                50   \n5              512            3                25   \n6              300            5                25   \n7              300            5                25   \n8              300            5                25   \n9              300            5                25   \n10             300            4                25   \n11             300            5                25   \n12             300            5                25   \n13             300            4                20   \n14             300            4                10   \n15             256            4                10   \n16             256            4                10   \n\n                              changed hyperParameters  \n0   ['model architecture', 'batch size', 'optimize...  \n1                         ['batch size', 'optimizer']  \n2   ['model architecture', 'batch size', 'window_s...  \n3                              ['model architecture']  \n4   ['model architecture', 'batch size', 'embeddin...  \n5   ['model architecture', 'embedding_size', 'no n...  \n6   ['model architecture', 'batch size', 'embeddin...  \n7                                       ['optimizer']  \n8                ['model architecture', 'batch size']  \n9                                       ['optimizer']  \n10  ['model architecture', 'batch size', 'window_s...  \n11              ['model architecture', 'window_size']  \n12                                      ['optimizer']  \n13                ['window_size', 'no noise samples']  \n14                  ['optimizer', 'no noise samples']  \n15           ['model architecture', 'embedding_size']  \n16                                      ['optimizer']  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch idx from</th>\n      <th>model architecture</th>\n      <th>batch size</th>\n      <th>optimizer</th>\n      <th>embedding_size</th>\n      <th>window_size</th>\n      <th>no noise samples</th>\n      <th>changed hyperParameters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(4634...</td>\n      <td>128</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>256</td>\n      <td>5</td>\n      <td>25</td>\n      <td>['model architecture', 'batch size', 'optimize...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(4634...</td>\n      <td>512</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>256</td>\n      <td>5</td>\n      <td>25</td>\n      <td>['batch size', 'optimizer']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1033...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>256</td>\n      <td>3</td>\n      <td>25</td>\n      <td>['model architecture', 'batch size', 'window_s...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(2338...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>256</td>\n      <td>3</td>\n      <td>25</td>\n      <td>['model architecture']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>27</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(2338...</td>\n      <td>512</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>128</td>\n      <td>3</td>\n      <td>50</td>\n      <td>['model architecture', 'batch size', 'embeddin...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>30</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(2338...</td>\n      <td>512</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>512</td>\n      <td>3</td>\n      <td>25</td>\n      <td>['model architecture', 'embedding_size', 'no n...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>40</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(3811...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>300</td>\n      <td>5</td>\n      <td>25</td>\n      <td>['model architecture', 'batch size', 'embeddin...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>51</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(3811...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>300</td>\n      <td>5</td>\n      <td>25</td>\n      <td>['optimizer']</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>52</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(3815...</td>\n      <td>512</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>300</td>\n      <td>5</td>\n      <td>25</td>\n      <td>['model architecture', 'batch size']</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>52</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(3815...</td>\n      <td>512</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>300</td>\n      <td>5</td>\n      <td>25</td>\n      <td>['optimizer']</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>77</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(2694...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>300</td>\n      <td>4</td>\n      <td>25</td>\n      <td>['model architecture', 'batch size', 'window_s...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>88</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1794...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>300</td>\n      <td>5</td>\n      <td>25</td>\n      <td>['model architecture', 'window_size']</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>107</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1794...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>300</td>\n      <td>5</td>\n      <td>25</td>\n      <td>['optimizer']</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>112</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1794...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>300</td>\n      <td>4</td>\n      <td>20</td>\n      <td>['window_size', 'no noise samples']</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>114</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1794...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>300</td>\n      <td>4</td>\n      <td>10</td>\n      <td>['optimizer', 'no noise samples']</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>114</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1794...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>256</td>\n      <td>4</td>\n      <td>10</td>\n      <td>['model architecture', 'embedding_size']</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>128</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1794...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>256</td>\n      <td>4</td>\n      <td>10</td>\n      <td>['optimizer']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_intervals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- best results at the sentiment analysis section was on the last two intervals of training starting from epoch 115"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "    epoch idx from                                 model architecture  \\\n15             114  SkipGram(\\n  (input_embedding): Embedding(1794...   \n16             128  SkipGram(\\n  (input_embedding): Embedding(1794...   \n\n    batch size                                          optimizer  \\\n15         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n16         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n\n    embedding_size  window_size  no noise samples  \\\n15             256            4                10   \n16             256            4                10   \n\n                     changed hyperParameters  \n15  ['model architecture', 'embedding_size']  \n16                             ['optimizer']  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch idx from</th>\n      <th>model architecture</th>\n      <th>batch size</th>\n      <th>optimizer</th>\n      <th>embedding_size</th>\n      <th>window_size</th>\n      <th>no noise samples</th>\n      <th>changed hyperParameters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>114</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1794...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>256</td>\n      <td>4</td>\n      <td>10</td>\n      <td>['model architecture', 'embedding_size']</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>128</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1794...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>256</td>\n      <td>4</td>\n      <td>10</td>\n      <td>['optimizer']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_intervals[-2:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "epochs_data=pd.read_csv(train_data_dir+'/epochs_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "     Train Loss  no train rows  Time taken (M)        Date      Time\n115    3.054564        5189632            5.68  07/12/2022  13:25:00\n116    3.007079        5189632            5.77  07/12/2022  13:31:00\n117    2.980684        5189632            6.05  07/12/2022  13:37:00\n118    2.963794        5189632            6.22  07/12/2022  13:43:00\n119    2.952011        5189632            6.31  07/12/2022  13:49:00\n120    2.943582        5189632            6.40  07/12/2022  13:55:00\n121    2.936728        5189632            6.50  07/12/2022  14:02:00\n122    2.931645        5189632            6.44  07/12/2022  14:08:00\n123    2.927267        5189632            6.50  07/12/2022  14:15:00\n124    2.923833        5189632            6.54  07/12/2022  14:21:00\n125    2.921148        5189632            6.68  07/12/2022  14:28:00\n126    2.918595        5189632            6.70  07/12/2022  14:35:00\n127    2.867656        5189632            5.39  07/12/2022  14:45:00\n128    2.858568        5189632            5.49  07/12/2022  14:50:00\n129    2.853979        5189632            5.68  07/12/2022  14:56:00\n130    2.851100        5189632            5.81  07/12/2022  15:02:00\n131    2.848612        5189632            6.00  07/12/2022  15:08:00\n132    2.846599        5189632            6.17  07/12/2022  15:14:00",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Train Loss</th>\n      <th>no train rows</th>\n      <th>Time taken (M)</th>\n      <th>Date</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>115</th>\n      <td>3.054564</td>\n      <td>5189632</td>\n      <td>5.68</td>\n      <td>07/12/2022</td>\n      <td>13:25:00</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>3.007079</td>\n      <td>5189632</td>\n      <td>5.77</td>\n      <td>07/12/2022</td>\n      <td>13:31:00</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>2.980684</td>\n      <td>5189632</td>\n      <td>6.05</td>\n      <td>07/12/2022</td>\n      <td>13:37:00</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>2.963794</td>\n      <td>5189632</td>\n      <td>6.22</td>\n      <td>07/12/2022</td>\n      <td>13:43:00</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>2.952011</td>\n      <td>5189632</td>\n      <td>6.31</td>\n      <td>07/12/2022</td>\n      <td>13:49:00</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>2.943582</td>\n      <td>5189632</td>\n      <td>6.40</td>\n      <td>07/12/2022</td>\n      <td>13:55:00</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>2.936728</td>\n      <td>5189632</td>\n      <td>6.50</td>\n      <td>07/12/2022</td>\n      <td>14:02:00</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>2.931645</td>\n      <td>5189632</td>\n      <td>6.44</td>\n      <td>07/12/2022</td>\n      <td>14:08:00</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>2.927267</td>\n      <td>5189632</td>\n      <td>6.50</td>\n      <td>07/12/2022</td>\n      <td>14:15:00</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>2.923833</td>\n      <td>5189632</td>\n      <td>6.54</td>\n      <td>07/12/2022</td>\n      <td>14:21:00</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>2.921148</td>\n      <td>5189632</td>\n      <td>6.68</td>\n      <td>07/12/2022</td>\n      <td>14:28:00</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>2.918595</td>\n      <td>5189632</td>\n      <td>6.70</td>\n      <td>07/12/2022</td>\n      <td>14:35:00</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>2.867656</td>\n      <td>5189632</td>\n      <td>5.39</td>\n      <td>07/12/2022</td>\n      <td>14:45:00</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>2.858568</td>\n      <td>5189632</td>\n      <td>5.49</td>\n      <td>07/12/2022</td>\n      <td>14:50:00</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>2.853979</td>\n      <td>5189632</td>\n      <td>5.68</td>\n      <td>07/12/2022</td>\n      <td>14:56:00</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>2.851100</td>\n      <td>5189632</td>\n      <td>5.81</td>\n      <td>07/12/2022</td>\n      <td>15:02:00</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>2.848612</td>\n      <td>5189632</td>\n      <td>6.00</td>\n      <td>07/12/2022</td>\n      <td>15:08:00</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>2.846599</td>\n      <td>5189632</td>\n      <td>6.17</td>\n      <td>07/12/2022</td>\n      <td>15:14:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs_data[115:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 720x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_data[115:].plot(y='Train Loss',figsize=(10,5),color='red')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "    epoch idx from                                 model architecture  \\\n15             114  SkipGram(\\n  (input_embedding): Embedding(1794...   \n16             128  SkipGram(\\n  (input_embedding): Embedding(1794...   \n\n    batch size                                          optimizer  \\\n15         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n16         256  Adam (\\nParameter Group 0\\n    amsgrad: False\\...   \n\n    embedding_size  window_size  no noise samples  \\\n15             256            4                10   \n16             256            4                10   \n\n                     changed hyperParameters  \n15  ['model architecture', 'embedding_size']  \n16                             ['optimizer']  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch idx from</th>\n      <th>model architecture</th>\n      <th>batch size</th>\n      <th>optimizer</th>\n      <th>embedding_size</th>\n      <th>window_size</th>\n      <th>no noise samples</th>\n      <th>changed hyperParameters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>114</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1794...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>256</td>\n      <td>4</td>\n      <td>10</td>\n      <td>['model architecture', 'embedding_size']</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>128</td>\n      <td>SkipGram(\\n  (input_embedding): Embedding(1794...</td>\n      <td>256</td>\n      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n      <td>256</td>\n      <td>4</td>\n      <td>10</td>\n      <td>['optimizer']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_intervals[-2:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- at epoch 128 I changed the learn rate from 0.001 to 0.0001 and the loss decreased as you see"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "oDXvixkEHwdF"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}